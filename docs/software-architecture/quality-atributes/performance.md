---
sidebar_position: 5
---

# Performance

## Principles

It's about time.

Performance is all about time. The time it takes for the system to respond to a certain event that arrives to it, in practice that event could be an interrupt, a user action, a request from another system, and so on. **Performance is concerned with the timing constraints of responding to events**.

Through the years, performance was the main focus of the advances in software architecture, and so tradeoffs on virtually all other quality attributes were made to gain more performance. These days, because of advancements on hardware, modern architectures tend to focus more on other quality attributes as well, not sacrificing it all for performance.

**Every system has performance requirements**. Even systems that don't seem like they need to be performant have performance requirements. As an example, think of a simple text editor, it may seem at first that there aren't any performance requirements, but the system would become unusable if it took more than a few seconds for letters to show up on the screen after you type them on the keyboard.

Performance is usually linked with scalability - that is, as the system grows, performance should be maintained - which, in turn, is linked with [modifiability](./modifiability). As we've seen, [availability](./availability) is also linked with performance, because we can't really tell if the system is unavailable or if it's just taking a lot of time to respond.

:::note

### Concurrency

Concurrency is one of the most important concepts that an architecture must understand, and it's one of the least-taught in the field. Allowing computations to run in parallel can improve performance tremendously, specially if processes tend to get blocked for a certain time.

The main issue with concurrency usually is it's added complexity of writing safe code that prevents deadlocks and/or race conditions. However, the architect must not refrain from concurrency solely based on that, if he notices that concurrency can fit the architecture and bring value, he must identify the critical sections of the code, and make those concurrency safe
:::

## General Scenario

A performance scenario begins with an event arriving at the system. An arrival pattern for events is characterized as _periodic_, _stochastic_, and _sporadic_:

- _periodic_: Periodic events arrive predictably at regular time intervals
- _stochastic_: Stochastic events arrive according to some probabilistic distribution
- _sporadic_: Sporadic events arrive in a pattern that is neither periodic or stochastic.

The response of the system to the a stimulus can be measure by the following:

- _Latency_: The time between the arrival of the event and the response.
- _Deadlines in processing_: The processing should occur before a certain deadline
- _Throughput_: The throughput of a system measures the number of transactions a system can process in a unit of time
- _Jitter_: The allowable variation in latency
- _Number of events not processed_: because the system was to busy to respond, it may 'ignore' events.

With that we can now go over the general scenario for performance:

- _Source of stimulus_: The trigger of the event, this can be internal or external (possible multiple) to the system.
- _Stimulus_: The event arrivals.
- _Artifact_: The entire system, or specific components of the system
- _Environment_: In which state the system is when events arrive? Normal mode, overloaded, operating in degraded mode, etc.
- _Response_: The system process (or fails to process) the arriving events
- _Response Measure_: The time it takes to process the arriving events. It can be measured by the parameters explained above: latency, throughput, jitter, etc.

| Portion of Scenario | Possible Values                                          |
| ------------------- | -------------------------------------------------------- |
| Source              | Internal or external to the system                       |
| Stimulus            | Arrival of a periodic, sporadic, or stochastic event     |
| Artifact            | System or one or more components in the system           |
| Environment         | Operational mode: normal, emergency, peak load, overload |
| Response            | Process events, change level of service                  |
| Response Measure    | Latency, deadline, throughput, jitter, miss rate         |

## Tactics

The goal of performance tactics is to generate a response to an event arriving at the system withing some time-based constraint. Two basic contributors to the response time are **processing time** and **blocked time**.

- _Processing Time_: The time it takes for the system to process the event and it's request. Processing consumes resources, which takes time. Different resources behave differently as they become more saturated.
- _Blocked Time_: A computation can be blocked by the contention of some shared resource, because the resource is unavailable, or the because the computation depends on results of other computations that are not yet available.
  - _Contention for resources_: Many resources can only be used by a client at a time, and so other clients will have to wait until for it to be available. The more contention for a resource, the more likelihood of latency being introduced.
  - _Availability of resources_: A computation may depend on the availability of a resource. If the resource is currently unavailable, the computation will not be able to proceed. **You must identify places where resource availability may cause a significant contribution to overall latency**.
  - _Dependency on other computation_: A computation may have to way because it must synchronize with the results of another computation, or it must wait for the results of a computation that it initiated.

With that in mind, we can turn on to our tactics categories. **We can either reduce demand for resources, or make the resources handle the demand more effectively**.

- _Control resource demand_. This tactic operates on the demand side to
  produce smaller demand on the resources that will have to service the
  events.
- _Manage resources_. This tactic operates on the response side to make the resources at hand work more effectively in handling the demands put to them.

### Control Resource Demand

One way to increase performance is to carefully manage the demand for resources. This can be done by reducing the number of events processed by enforcing a sampling rate, or by limiting the rate at which the system responds to
events. In addition, there are a number of techniques for ensuring that the resources that you do have are applied judiciously:

- **Manage sampling rate**. If it is possible to reduce the sampling frequency at which a stream of environmental data is captured, then demand can be reduced, typically with some attendant loss of fidelity. This is common in signal processing systems where, for example, different codecs can be chosen with different sampling rates and data formats. This design choice is made to maintain predictable levels of latency; you must decide whether having a lower fidelity but consistent stream of data is preferable to losing packets of data.
- **Limit event response**. When discrete events arrive at the system (or element) too rapidly to be processed, then the events must be queued until they can be processed. Because these events are discrete, it is typically not desirable to “downsample” them. In such a case, you may choose to process events only up to a set maximum rate, thereby ensuring more predictable processing when the events are actually processed. This tactic could be triggered by a queue size or processor utilization measure exceeding some warning level. If you adopt this tactic and it is unacceptable to lose any events, then you must ensure that your queues are large enough to handle the worst case. If, on the other hand, you choose to drop events, then you need to choose a policy for handling this situation: Do you log the dropped events, or simply ignore them? Do you notify other systems, users, or administrators?
- **Prioritize events**. If not all events are equally important, you can impose a priority scheme that ranks events according to how important it is to service them. If there are not enough resources available to service them when they arise, low-priority events might be ignored. Ignoring events consumes minimal resources (including time), and thus increases performance compared to a system that services all events all the time. For example, a building management system may raise a variety of alarms. Life-threatening alarms such as a fire alarm should be given higher priority than informational alarms such as a room is too cold.
- **Reduce overhead**. The use of intermediaries (so important for modifiability, as we saw in Chapter 7) increases the resources consumed in processing an event stream, and so removing them improves latency. This is a classic modifiability/performance tradeoff. Separation of concerns, another linchpin of modifiability, can also increase the processing overhead necessary to service an event if it leads to an event being serviced by a chain of components rather than a single component. The context switching and intercomponent communication costs add up, especially when the components are on different nodes on a network. A strategy for reducing computational overhead is to co-locate resources. Co-location may mean hosting cooperating components on the same processor to avoid the time delay of network communication; it may mean putting the resources in the same runtime software component to avoid even the expense of a subroutine call. A special case of reducing computational overhead is to perform a periodic cleanup of resources that have become inefficient. For example, hash tables and virtual memory maps may require recalculation and reinitialization. Another common strategy is to execute single-threaded servers (for simplicity and avoiding contention) and split workload across them.
- **Bound execution times**. Place a limit on how much execution time is used to respond to an event. For iterative, data-dependent algorithms, limiting the number of iterations is a method for bounding execution times. The cost is usually a less accurate computation. If you adopt this tactic, you will need to assess its effect on accuracy and see if the result is “good enough.” This resource management tactic is frequently paired with the manage sampling rate tactic.
- **Increase resource efficiency**. Improving the algorithms used in critical areas will decrease latency.

### Manage Resources

Even if the demand for resources is not controllable, the management of these resources can be. Sometimes one resource can be traded for another. For example, intermediate data may be kept in a cache or it may be regenerated depending on time and space resource availability. This tactic is usually applied to the processor but is also effective when applied to other resources such as a disk. Here are some resource management tactics:

- **Increase resources**. Faster processors, additional processors, additional memory, and faster networks all have the potential for reducing latency. Cost is usually a consideration in the choice of resources, but increasing the resources is definitely a tactic to reduce latency and in many cases is the cheapest way to get immediate improvement.
- **Introduce concurrency**. If requests can be processed in parallel, the blocked time can be reduced. Concurrency can be introduced by processing different streams of events on different threads or by creating additional threads to process different sets of activities. Once concurrency has been introduced, scheduling policies can be used to achieve the goals you find desirable. Different scheduling policies may maximize fairness (all requests get equal time), throughput (shortest time to finish first), or other goals. (See the sidebar.)
- **Maintain multiple copies of computations**. Multiple servers in a client-server pattern are replicas of computation. The purpose of replicas is to reduce the contention that would occur if all computations took place on a single server. A load balancer is a piece of software that assigns new work to one of the available duplicate servers; criteria for assignment vary but can be as simple as round-robin or assigning the next request to the least busy server.
- **Maintain multiple copies of data**. Caching is a tactic that involves keeping copies of data (possibly one a subset of the other) on storage with different access speeds. The different access speeds may be inherent (memory versus secondary storage) or may be due to the necessity for network communication. Data replication involves keeping separate copies of the data to reduce the contention from multiple simultaneous accesses. Because the data being cached or replicated is usually a copy of existing data, keeping the copies consistent and synchronized becomes a responsibility that the system must assume. Another responsibility is to choose the data to be cached. Some caches operate by merely keeping copies of whatever was recently requested, but it is also possible to predict users’ future requests based on patterns of behavior, and begin the calculations or prefetches necessary to comply with those requests before the user has made them.
- **Bound queue sizes**. This controls the maximum number of queued arrivals and consequently the resources used to process the arrivals. If you adopt this tactic, you need to adopt a policy for what happens when the queues overflow and decide if not responding to lost events is acceptable. This tactic is frequently paired with the limit event response tactic.
- **Schedule resources**. Whenever there is contention for a resource, the resource must be scheduled. Processors are scheduled, buffers are scheduled, and networks are scheduled. Your goal is to understand the characteristics of each resource’s use and choose the scheduling strategy that is compatible with it.

## Checklist

| Category                             | Checklist                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Allocation of Responsibilities       | Determine the system’s responsibilities that will involve heavy loading, have time-critical response requirements, are heavily used, or impact portions of the system where heavy loads or time-critical events occur. <br/> For those responsibilities, identify the processing requirements of each responsibility, and determine whether they may cause bottlenecks. Also, identify additional responsibilities to recognize and process requests appropriately, including<ul><li> Responsibilities that result from a thread of control crossing process or processor boundaries </li> <li> Responsibilities to manage the threads of control—allocation and deallocation of threads, maintaining thread pools, and so forth </li> <li> Responsibilities for scheduling shared resources or managing performance-related artifacts such as queues, buffers, and caches</li> </ul> For the responsibilities and resources you identified, ensure that the required performance response can be met (perhaps by building a performance model to help in the evaluation). |
| Coordination Model                   | Determine the elements of the system that must coordinate with each other—directly or indirectly—and choose communication and coordination mechanisms that do the following: <ul> <li> Support any introduced concurrency (for example, is it thread safe?), event prioritization, or scheduling strategy </li> <li> Ensure that the required performance response can be delivered </li> <li> Can capture periodic, stochastic, or sporadic event arrivals, as needed </li> <li> Have the appropriate properties of the communication mechanisms; for example, stateful, stateless, synchronous, asynchronous, guaranteed delivery, throughput, or latency </li> </ul>                                                                                                                                                                                                                                                                                                                                                                                                    |
| Data Model                           | Determine those portions of the data model that will be heavily loaded, have time-critical response requirements, are heavily used, or impact portions of the system where heavy loads or time-critical events occur. <br/>For those data abstractions, determine the following: <ul> <li> Whether maintaining multiple copies of key data would benefit performance</li> <li> Whether partitioning data would benefit performance </li> <li> Whether reducing the processing requirements for the creation, initialization, persistence, manipulation, translation, or destruction of the enumerated data abstractions is possible </li> <li> Whether adding resources to reduce bottlenecks for the creation, initialization, persistence, manipulation, translation, or destruction of the enumerated data abstractions is feasible</li> </ul>                                                                                                                                                                                                                          |
| Mapping among Architectural Elements | Where heavy network loading will occur, determine whether co-locating some components will reduce loading and improve overall efficiency. <br/>Ensure that components with heavy computation requirements are assigned to processors with the most processing capacity. <br/>Determine where introducing concurrency (that is, allocating a piece of functionality to two or more copies of a component running simultaneously) is feasible and has a significant positive effect on performance. Determine whether the choice of threads of control and their associated responsibilities introduces bottlenecks.                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Resource Management                  | Determine which resources in your system are critical for performance. For these resources, ensure that they will be monitored and managed under normal and overloaded system operation. For example: <ul> <li>System elements that need to be aware of, and manage, time and other performance-critical resources </li> <li>Process/thread models </li> <li>Prioritization of resources and access to resources </li> <li>Scheduling and locking strategies </li> <li>Deploying additional resources on demand to meet increased loads</li> </ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Binding Time                         | For each element that will be bound after compile time, determine the following: <ul> <li> Time necessary to complete the binding </li> <li> Additional overhead introduced by using the late binding mechanism</li> </ul>Ensure that these values do not pose unacceptable performance penalties on the system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Choice of Technology                 | Will your choice of technology let you set and meet hard, realtime deadlines? Do you know its characteristics under load and its limits?<br/> Does your choice of technology give you the ability to set the following:<ul><li>Scheduling policy </li><li>Priorities </li><li>Policies for reducing demand </li><li>Allocation of portions of the technology to processors </li><li>Other performance-related parameters</li></ul>Does your choice of technology introduce excessive overhead for heavily used operations?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
